import { useState, useMemo, useEffect, useRef } from "react";
import { App } from "../App";
import { useChat } from "../hooks/use-chat";
import { ChatMessage } from "../components/ChatMessage";
import { appConfig } from "../../config.browser";
import { Welcome } from "../components/Welcome";
import { ImageAnnotatorClient } from '@google-cloud/vision';
import { SpeechClient } from '@google-cloud/speech';


export default function Index() {
  const vision = require('@google-cloud/vision');
  const speech = require('@google-cloud/speech');

  // Obtén las claves de API desde las variables de entorno
  const googleVisionApiKey = process.env.GOOGLE_VISION_API_KEY;
  const googleSpeechToTextApiKey = process.env.GOOGLE_SPEECH_TO_TEXT_API_KEY;

  // Configurar el cliente de Vision IA de Google
  const visionClient = new vision.ImageAnnotatorClient({
    credentials: {
      private_key: googleVisionApiKey,
      client_email: 'travelmate@golden-ego-401716.iam.gserviceaccount.com',
    },
  });

  // Configurar el cliente de Speech-to-Text de Google
  const speechClient = new speech.SpeechClient({
    credentials: {
      private_key: googleSpeechToTextApiKey,
      client_email: 'travelmate@golden-ego-401716.iam.gserviceaccount.com',
    },
  });

  // Ahora puedes usar visionClient y speechClient para hacer solicitudes a las respectivas APIs.
  // Por ejemplo, aquí tienes un ejemplo de cómo usar Google Vision API para analizar una imagen:

  async function analyzeImage(imagePath) {
    const [result] = await visionClient.textDetection(imagePath);
    const text = result.fullTextAnnotation.text;
    console.log('Texto en la imagen:', text);
  }

  // Llama a la función de análisis de imagen
  //const imagePath = 'path/to/your/image.jpg';
  //analyzeImage(imagePath);


  // Función para transcribir un archivo de audio en texto
  async function transcribeAudio(audioPath) {
    const config = {
      encoding: 'LINEAR16',
      sampleRateHertz: 16000,
      languageCode: 'es-US', // Cambia esto al código del idioma de tu audio si es diferente
    };

    const audio = {
      content: audioPath,
    };

    const request = {
      audio: audio,
      config: config,
    };

    // Realiza la transcripción
    const [response] = await speechClient.recognize(request);
    const transcription = response.results
      .map(result => result.alternatives[0].transcript)
      .join('\n');

    console.log('Texto en el audio:', transcription);
  }

  // Llama a la función de transcripción con la ruta al archivo de audio
  //const audioPath = 'path/to/your/audio.wav';
  //transcribeAudio(audioPath);






  // The content of the box where the user is typing
  const [message, setMessage] = useState<string>("");

  // This hook is responsible for managing the chat and communicating with the
  // backend
  const { currentChat, chatHistory, sendMessage, cancel, state, clear } =
    useChat();

  // This is the message that is currently being generated by the AI
  const currentMessage = useMemo(() => {
    return { content: currentChat ?? "", role: "assistant" } as const;
  }, [currentChat]);

  // This is a ref to the bottom of the chat history. We use it to scroll
  // to the bottom when a new message is added.
  const bottomRef = useRef<HTMLDivElement>(null);

  useEffect(() => {
    scrollToBottom();
  }, [currentChat, chatHistory, state]);

  const scrollToBottom = () => {
    bottomRef.current?.scrollIntoView({ behavior: "smooth" });
  };

  // This is a ref to the input box. We use it to focus the input box when the
  // user clicks on the "Send" button.
  const inputRef = useRef<HTMLInputElement>(null);
  const focusInput = () => {
    inputRef.current?.focus();
  };

  useEffect(() => {
    focusInput();
  }, [state]);

  const handleImageUpload = async (image) => {
    const client = new ImageAnnotatorClient();
    const [result] = await client.textDetection(image.path);
    const text = result.fullTextAnnotation.text;
    sendMessage(`Texto en la imagen: ${text}`, chatHistory);
  };

  const handleAudioUpload = async (audio) => {
    const client = new SpeechClient();
    const config = {
      encoding: 'LINEAR16',
      sampleRateHertz: 16000,
      languageCode: 'es-US',
    };

    const audioBytes = audio.buffer.toString('base64');
    const audioData = {
      content: audioBytes,
    };

    const [response] = await client.recognize(config, audioData);
    const transcription = response.results
      .map((result) => result.alternatives[0].transcript)
      .join('\n');
    sendMessage(`Texto en el audio: ${transcription}`, chatHistory);
  };

  const handleImageChange = (e) => {
    const image = e.target.files[0];
    handleImageUpload(image);
  };

  const handleAudioChange = (e) => {
    const audio = e.target.files[0];
    handleAudioUpload(audio);
  };

  return (
    <App title="TravelMate">
      <main className="bg-white md:rounded-lg md:shadow-md p-6 w-full h-full flex flex-col">
        <section className="overflow-y-auto flex-grow mb-4 pb-8">
          <div className="flex flex-col space-y-4">
            {chatHistory.length === 0 ? (
              <>
                <Welcome />
                <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-4">
                  {appConfig.samplePhrases.map((phrase) => (
                    <button
                      key={phrase}
                      onClick={() => sendMessage(phrase, chatHistory)}
                      className="bg-gray-100 border-gray-300 border-2 rounded-lg p-4"
                    >
                      {phrase}
                    </button>
                  ))}
                </div>
                <div className="flex justify-center">
                  <p className="text-sm text-gray-500 mt-5">
                    Built with 🤖{" "}
                    <a
                      className="underline"
                      href="https://github.com/ascorbic/daneel"
                    >
                      Daneel
                    </a>
                  </p>
                </div>
              </>
            ) : (
              chatHistory.map((chat, i) => (
                <ChatMessage key={i} message={chat} />
              ))
            )}

            {currentChat ? <ChatMessage message={currentMessage} /> : null}
          </div>

          <div ref={bottomRef} />
        </section>
        <div className="flex items-center justify-center h-20">
          {state === "idle" ? null : (
            <button
              className="bg-gray-100 text-gray-900 py-2 px-4 my-8"
              onClick={cancel}
            >
              Detener generación
            </button>
          )}
        </div>
        <section className="bg-gray-100 rounded-lg p-2">
          <form
            className="flex"
            onSubmit={(e) => {
              e.preventDefault();
              sendMessage(message, chatHistory);
              setMessage("");
            }}
          >
            {chatHistory.length > 1 ? (
              <button
                className="bg-gray-100 text-gray-600 py-2 px-4 rounded-l-lg"
                type="button"
                onClick={(e) => {
                  e.preventDefault();
                  clear();
                  setMessage("");
                }}
              >
                Limpiar
              </button>
            ) : null}


            <input
              type="text"
              ref={inputRef}
              className="w-full rounded-l-lg p-2 outline-none"
              placeholder={state == "idle" ? "Type your message..." : "..."}
              value={message}
              onChange={(e) => setMessage(e.target.value)}
              disabled={state !== "idle"}
            />
            {state === "idle" ? (
              <button
                className="bg-green-700 text-white font-bold py-2 px-4 rounded-r-lg"
                type="submit"
              >
                Enviar
              </button>
            ) : null}
          </form>
        </section>
      </main>
    </App>
  );
}
